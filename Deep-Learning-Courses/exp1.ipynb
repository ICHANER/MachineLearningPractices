{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 一、Pytorch基本操作考察"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b52a3e98a1e57a69"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.1 使用Tensor初始化一个 1x3 的矩阵 M 和一个 2x1 的矩阵 N，对两矩阵进行减法操作（要求实现三种不同的形式），给出结果并分析三种方式的不同（如果出现报错，分析报错的原因），同时需要指出在计算过程中发生了什么."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b62e29ad8d0d4904"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前设备： mps\n"
     ]
    }
   ],
   "source": [
    "# Step 1: 导入Pytorch\n",
    "import torch\n",
    "\n",
    "# 检查设备可用性\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # 如果支持cuda，则选择cuda\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # 如果支持Apple M系芯片，则选择mps\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # 否则选择cpu\n",
    "print(\"当前设备：\", device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T08:50:05.060032Z",
     "start_time": "2024-03-28T08:50:05.057040Z"
    }
   },
   "id": "bffa8d873c2c32e6"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6477,  0.7037, -1.1246]])\n",
      "tensor([[ 0.3003],\n",
      "        [-0.1905]])\n"
     ]
    }
   ],
   "source": [
    "# Step 2: 初始化矩阵M和N\n",
    "M = torch.randn(1,3)\n",
    "N = torch.randn(2,1)\n",
    "print(M)\n",
    "print(N)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T08:50:06.446019Z",
     "start_time": "2024-03-28T08:50:06.442032Z"
    }
   },
   "id": "b5bb0cc622e0778f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "为了更好地分析结果，我们给M、N张量赋予特值"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9de3cce86c4e116d"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3]], device='mps:0')\n",
      "tensor([[1],\n",
      "        [2]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "M = torch.tensor([[1,2,3]])\n",
    "N = torch.tensor([[1],[2]])\n",
    "\n",
    "# 将张量发送到所选设备\n",
    "M = M.to(device)\n",
    "N = N.to(device)\n",
    "\n",
    "print(M)\n",
    "print(N)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T08:50:08.177409Z",
     "start_time": "2024-03-28T08:50:08.168577Z"
    }
   },
   "id": "b160fc7c6d7777ce"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2],\n",
      "        [-1,  0,  1]], device='mps:0')\n",
      "tensor([[ 0,  1,  2],\n",
      "        [-1,  0,  1]], device='mps:0')\n",
      "tensor([[ 0,  1,  2],\n",
      "        [-1,  0,  1]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# Step 3: 对两矩阵进行减法操作 （三种实现形式）\n",
    "# 减法形式一：\n",
    "print(M-N) \n",
    "# 减法形式二：\n",
    "print(M.sub(N))\n",
    "# 减法形式三：\n",
    "print(torch.sub(M,N))  # print(torch.subtract(M,N)) # 同上"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T08:50:08.616451Z",
     "start_time": "2024-03-28T08:50:08.594935Z"
    }
   },
   "id": "669592a6594af6c1"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [1, 3] doesn't match the broadcast shape [2, 3]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[67], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 减法形式四：\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(M\u001B[38;5;241m.\u001B[39msub_(N))\n",
      "\u001B[0;31mRuntimeError\u001B[0m: output with shape [1, 3] doesn't match the broadcast shape [2, 3]"
     ]
    }
   ],
   "source": [
    "# 减法形式四：\n",
    "print(M.sub_(N))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T08:50:08.814780Z",
     "start_time": "2024-03-28T08:50:08.758927Z"
    }
   },
   "id": "eecbaf6c9b28322a"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) tensor([4, 5, 6])\n",
      "tensor([-3, -3, -3]) \t tensor([1, 2, 3]) \t tensor([4, 5, 6])\n",
      "tensor([-3, -3, -3]) \t tensor([-3, -3, -3]) \t tensor([4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "Mx = torch.tensor([1,2,3])\n",
    "Nx = torch.tensor([4,5,6])\n",
    "print(Mx,Nx)\n",
    "print(Mx.sub(Nx),'\\t',Mx,'\\t',Nx)\n",
    "print(Mx.sub_(Nx),'\\t',Mx,'\\t',Nx)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T08:50:08.966147Z",
     "start_time": "2024-03-28T08:50:08.940463Z"
    }
   },
   "id": "d5696a20b8218ed8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "由上我们可见：使用张量的sub_方法，提示报错，原因是M、N的形状不同，无法广播，无法进行减法操作。\n",
    "重新定义相同大小的Mx和Nx张量，并使用sub_方法进行减法操作。\n",
    "可以发现，sub_是在在原张量上进行减法操作，不会产生新的张量，而是直接修改原张量的值；而sub方法则会返回一个新的张量，不会修改原张量的值。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2bec4567119b70b1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "&ensp; 由Step3的结果可以发现，三种方法的输出结果都是相同的，都是对应元素相减得到的矩阵。在计算过程中，Pytorch会自动进行广播操作，将1x3的矩阵M和2x1的矩阵N进行广播，使得它们的形状相同，从而进行元素级别的减法操作。\n",
    "\n",
    "其计算过程为：\n",
    "&ensp; [1,2,3]会被广播为[[1,2,3],[1,2,3]]\n",
    "&ensp; [[1],[2]]会被广播为[[1,1,1],[2,2,2]]\n",
    "&ensp; [[1,2,3],[1,2,3]] - [[1,1,1],[2,2,2]] = [[0,1,2],[-1,0,1]]\n",
    "&ensp; 即得到了以上的结果。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68a4caff65db0573"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.2 ① 利用 Tensor 创建两个大小分别 3x2 和 4x2 的随机数矩阵 P 和 Q，要求服从均值为 0，标准差0.01为的正态分布；② 对第二步得到的矩阵 Q 进行形状变换得到 Q 的转置 Q<sup>T</sup> ；③ 对上述得到的矩阵 P 和矩阵 Q<sup>T</sup>求矩阵相乘"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82129a0f5b775d48"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0032, -0.0106],\n",
      "        [-0.0138,  0.0139],\n",
      "        [-0.0042,  0.0075]], dtype=torch.float64)\n",
      "tensor([[-3.4224e-05, -1.3588e-02],\n",
      "        [-1.5247e-02, -5.9312e-03],\n",
      "        [ 7.3446e-03,  3.1417e-03],\n",
      "        [-1.5942e-02, -1.8460e-03]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Step1: N(0,0.01)创建\n",
    "# 法一：\n",
    "P = torch.randn(3,2)*0.01\n",
    "Q = torch.randn(4,2)*0.01\n",
    "# 法二：\n",
    "import numpy as np\n",
    "P = torch.tensor(np.random.normal(0,0.01,size=(3,2)))\n",
    "Q = torch.tensor(np.random.normal(0,0.01,size=(4,2)))\n",
    "print(P)\n",
    "print(Q)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T09:10:10.841730Z",
     "start_time": "2024-03-28T09:10:10.825439Z"
    }
   },
   "id": "8d5080a6c21ba37c"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.4224e-05, -1.5247e-02,  7.3446e-03, -1.5942e-02],\n",
      "        [-1.3588e-02, -5.9312e-03,  3.1417e-03, -1.8460e-03]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: 对Q进行转置\n",
    "Qt = Q.t()\n",
    "print(Qt)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T09:15:54.060552Z",
     "start_time": "2024-03-28T09:15:54.013419Z"
    }
   },
   "id": "b08d89019ba6f8e4"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4332e-04,  1.3260e-05, -9.3918e-06, -3.2112e-05],\n",
      "        [-1.8853e-04,  1.2864e-04, -5.8008e-05,  1.9509e-04],\n",
      "        [-1.0175e-04,  1.8909e-05, -6.9747e-06,  5.2432e-05]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(torch.mm(P,Qt))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T09:16:16.451993Z",
     "start_time": "2024-03-28T09:16:16.437448Z"
    }
   },
   "id": "e1683cde3d3f292c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.3 给定公式 y3 = y1 + y2 = x<sup>2</sup> + x<sup>3</sup>，且 x=1。利用学习所得到的Tensor的相关知识，求y3对x的梯度，即dy3/dx。 <br>&ensp;&ensp;要求在计算过程中，在计算x<sup>3</sup>时中断梯度的追踪，观察结果并进行原因分析.<br>提示：可使用 with torch.no_grad(),举例：<br> with torch.no_grad():<br>&ensp;&ensp;y=x*5"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1fbcfa54575f560"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# 计算 y1\n",
    "y1 = x ** 2\n",
    "\n",
    "# 中断梯度追踪\n",
    "with torch.no_grad():\n",
    "    # 计算 y2\n",
    "    y2 = x ** 3\n",
    "\n",
    "# 计算 y3\n",
    "y3 = y1 + y2\n",
    "\n",
    "# 计算梯度dy3/dx，反向传播\n",
    "y3.backward()\n",
    "\n",
    "#输出x的梯度值\n",
    "print(x.grad)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T09:32:38.384796Z",
     "start_time": "2024-03-28T09:32:38.378253Z"
    }
   },
   "id": "9e3ea3ad2f4308b7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "对如上结果进行分析：\n",
    "如果不中断y2=x^3的梯度传递，则最终 y3对x 的梯度值应为5，而实际为 2，则说明 y2 在计算过程中没有被追踪梯度，因此 y3 的梯度只与 y1 相关。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4e15d44dbf3ae29"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 二、动手实现 logistic 回归"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38c3ad8b5f8f2fe3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 从 0 实现"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e12c66d730ccbb5e"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.710352\n",
      "epoch 2, loss 0.712597\n",
      "epoch 3, loss 0.695746\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Model:\n",
    "    # 定义模型\n",
    "    def logistic(self, X, w, b):\n",
    "        y = self.sigmoid(torch.matmul(X, w) + b)  # 通过sigmoid函数将线性回归的输出转换为概率\n",
    "        return y\n",
    "\n",
    "    # 定义激活函数\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "    # 定义损失函数（二元交叉熵损失/对数损失函数）\n",
    "    def loss(self, y_pred, y_true):\n",
    "        return -torch.mean(y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred))\n",
    "\n",
    "    # 定义优化算法 SGD\n",
    "    def sgd(self, w, b, lr):\n",
    "        with torch.no_grad():\n",
    "            w -= lr * w.grad\n",
    "            b -= lr * b.grad\n",
    "\n",
    "\n",
    "class Train:\n",
    "    def __init__(self, model, features, labels, lr, epochs):\n",
    "        self.model = model\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.w = torch.normal(0, 0.01, size=(features.shape[1], 1), requires_grad=True)  # 权重\n",
    "        self.b = torch.zeros(1, requires_grad=True)  # 偏置\n",
    "\n",
    "        # 读取数据集\n",
    "\n",
    "    def data_iter(self, batch_size, features, labels):\n",
    "        num_examples = len(features)\n",
    "        indices = list(range(num_examples))\n",
    "        np.random.shuffle(indices)\n",
    "        for i in range(0, num_examples, batch_size):\n",
    "            batch_indices = torch.tensor(indices[i: min(i + batch_size, num_examples)])\n",
    "            yield features[batch_indices], labels[batch_indices]\n",
    "\n",
    "    # 训练\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            for X, y in self.data_iter(batch_size=10, features=self.features, labels=self.labels):\n",
    "                y_pred = self.model.logistic(X, self.w, self.b)\n",
    "                l = Model.loss(model ,y_pred, y)\n",
    "\n",
    "                # l.backward()\n",
    "                l.sum().backward()\n",
    "                Model.sgd(model ,self.w, self.b, self.lr)\n",
    "            with torch.no_grad():\n",
    "                y_pred = self.model.logistic(features, self.w, self.b)\n",
    "                l = Model.loss(model ,y_pred, labels)\n",
    "                print(f'epoch {epoch + 1}, loss {float(l.mean()):f}')\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = self.model.logistic(X, self.w, self.b)  # 预测值\n",
    "        return y_pred\n",
    "\n",
    "    # 保存模型\n",
    "    def save_model(self, path):\n",
    "        torch.save({\n",
    "            'w': self.w,\n",
    "            'b': self.b\n",
    "        }, path)\n",
    "\n",
    "\n",
    "class DataProducer:\n",
    "    # 生成正态线性数据集\n",
    "    def synthetic_data(self, w, b, num_samples):\n",
    "        X = torch.normal(0, 1, (num_samples, len(w)))\n",
    "        y = torch.matmul(X, w) + b\n",
    "        y += torch.normal(0, 0.01, y.shape)\n",
    "        return X, y.reshape((-1, 1))\n",
    "\n",
    "    # 生成二分类数据集\n",
    "    def synthesize_data_binarization(self, num_examples):\n",
    "        n_data = torch.ones(num_examples, 2)  # 数据的基本形态 num_examples x 2\n",
    "\n",
    "        x1 = torch.normal(2 * n_data, 1)  # shape=(50, 2)\n",
    "        y1 = torch.zeros(num_examples)  # 类型0 shape=(50, 1)\n",
    "\n",
    "        x2 = torch.normal(-2 * n_data, 1)  # shape=(50, 2)\n",
    "        y2 = torch.ones(num_examples)  # 类型1 shape=(50, 1)\n",
    "\n",
    "        # 注意 x, y 数据的数据形式一定要像下面一样 (torch.cat 是合并数据)\n",
    "        x = torch.cat((x1, x2), 0).type(torch.FloatTensor)\n",
    "        y = torch.cat((y1, y2), 0).type(torch.FloatTensor)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 准备数据集\n",
    "    data_producer = DataProducer()\n",
    "    data = data_producer.synthesize_data_binarization(num_examples=3000)  # 生成1000个样本\n",
    "    features, labels = data  # 分别获取特征和标签\n",
    "\n",
    "    # 初始化模型参数\n",
    "    model = Model()\n",
    "\n",
    "    # 训练\n",
    "    batch_size = 100  # 每批数据集大小\n",
    "    train = Train(model, features, labels, lr=0.003, epochs=3)\n",
    "    train.train()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T13:00:02.562737Z",
     "start_time": "2024-03-28T13:00:02.145230Z"
    }
   },
   "id": "7990df69e262e6a8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 利用torch.nn实现"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b6820c46afaa1ef"
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T09:35:51.446800Z",
     "start_time": "2024-03-28T09:35:51.429493Z"
    }
   },
   "id": "7c8c734f302a4ac1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 三、动手实现 softmax 回归"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db3600f9f7fb11dd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 从0实现"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92fc1c1454281d21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "class softmax():\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec0fa5784014b7f2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 利用torch.nn实现"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3d4fab688298fe0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1243741758212ce"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
