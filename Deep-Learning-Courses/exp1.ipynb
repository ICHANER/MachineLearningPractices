{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 一、Pytorch基本操作考察"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b52a3e98a1e57a69"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.1 使用Tensor初始化一个 1x3 的矩阵 M 和一个 2x1 的矩阵 N，对两矩阵进行减法操作（要求实现三种不同的形式），给出结果并分析三种方式的不同（如果出现报错，分析报错的原因），同时需要指出在计算过程中发生了什么."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b62e29ad8d0d4904"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前设备： mps\n"
     ]
    }
   ],
   "source": [
    "# Step 1: 导入Pytorch\n",
    "import torch\n",
    "\n",
    "# 检查设备可用性\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # 如果支持cuda，则选择cuda\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # 如果支持Apple M系芯片，则选择mps\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # 否则选择cpu\n",
    "print(\"当前设备：\", device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T08:50:05.060032Z",
     "start_time": "2024-03-28T08:50:05.057040Z"
    }
   },
   "id": "bffa8d873c2c32e6"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6477,  0.7037, -1.1246]])\n",
      "tensor([[ 0.3003],\n",
      "        [-0.1905]])\n"
     ]
    }
   ],
   "source": [
    "# Step 2: 初始化矩阵M和N\n",
    "M = torch.randn(1,3)\n",
    "N = torch.randn(2,1)\n",
    "print(M)\n",
    "print(N)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T08:50:06.446019Z",
     "start_time": "2024-03-28T08:50:06.442032Z"
    }
   },
   "id": "b5bb0cc622e0778f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "为了更好地分析结果，我们给M、N张量赋予特值"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9de3cce86c4e116d"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3]], device='mps:0')\n",
      "tensor([[1],\n",
      "        [2]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "M = torch.tensor([[1,2,3]])\n",
    "N = torch.tensor([[1],[2]])\n",
    "\n",
    "# 将张量发送到所选设备\n",
    "M = M.to(device)\n",
    "N = N.to(device)\n",
    "\n",
    "print(M)\n",
    "print(N)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T08:50:08.177409Z",
     "start_time": "2024-03-28T08:50:08.168577Z"
    }
   },
   "id": "b160fc7c6d7777ce"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2],\n",
      "        [-1,  0,  1]], device='mps:0')\n",
      "tensor([[ 0,  1,  2],\n",
      "        [-1,  0,  1]], device='mps:0')\n",
      "tensor([[ 0,  1,  2],\n",
      "        [-1,  0,  1]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# Step 3: 对两矩阵进行减法操作 （三种实现形式）\n",
    "# 减法形式一：\n",
    "print(M-N) \n",
    "# 减法形式二：\n",
    "print(M.sub(N))\n",
    "# 减法形式三：\n",
    "print(torch.sub(M,N))  # print(torch.subtract(M,N)) # 同上"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T08:50:08.616451Z",
     "start_time": "2024-03-28T08:50:08.594935Z"
    }
   },
   "id": "669592a6594af6c1"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [1, 3] doesn't match the broadcast shape [2, 3]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[67], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 减法形式四：\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(M\u001B[38;5;241m.\u001B[39msub_(N))\n",
      "\u001B[0;31mRuntimeError\u001B[0m: output with shape [1, 3] doesn't match the broadcast shape [2, 3]"
     ]
    }
   ],
   "source": [
    "# 减法形式四：\n",
    "print(M.sub_(N))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T08:50:08.814780Z",
     "start_time": "2024-03-28T08:50:08.758927Z"
    }
   },
   "id": "eecbaf6c9b28322a"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) tensor([4, 5, 6])\n",
      "tensor([-3, -3, -3]) \t tensor([1, 2, 3]) \t tensor([4, 5, 6])\n",
      "tensor([-3, -3, -3]) \t tensor([-3, -3, -3]) \t tensor([4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "Mx = torch.tensor([1,2,3])\n",
    "Nx = torch.tensor([4,5,6])\n",
    "print(Mx,Nx)\n",
    "print(Mx.sub(Nx),'\\t',Mx,'\\t',Nx)\n",
    "print(Mx.sub_(Nx),'\\t',Mx,'\\t',Nx)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T08:50:08.966147Z",
     "start_time": "2024-03-28T08:50:08.940463Z"
    }
   },
   "id": "d5696a20b8218ed8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "由上我们可见：使用张量的sub_方法，提示报错，原因是M、N的形状不同，无法广播，无法进行减法操作。\n",
    "重新定义相同大小的Mx和Nx张量，并使用sub_方法进行减法操作。\n",
    "可以发现，sub_是在在原张量上进行减法操作，不会产生新的张量，而是直接修改原张量的值；而sub方法则会返回一个新的张量，不会修改原张量的值。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2bec4567119b70b1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "&ensp; 由Step3的结果可以发现，三种方法的输出结果都是相同的，都是对应元素相减得到的矩阵。在计算过程中，Pytorch会自动进行广播操作，将1x3的矩阵M和2x1的矩阵N进行广播，使得它们的形状相同，从而进行元素级别的减法操作。\n",
    "\n",
    "其计算过程为：\n",
    "&ensp; [1,2,3]会被广播为[[1,2,3],[1,2,3]]\n",
    "&ensp; [[1],[2]]会被广播为[[1,1,1],[2,2,2]]\n",
    "&ensp; [[1,2,3],[1,2,3]] - [[1,1,1],[2,2,2]] = [[0,1,2],[-1,0,1]]\n",
    "&ensp; 即得到了以上的结果。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68a4caff65db0573"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.2 ① 利用 Tensor 创建两个大小分别 3x2 和 4x2 的随机数矩阵 P 和 Q，要求服从均值为 0，标准差0.01为的正态分布；② 对第二步得到的矩阵 Q 进行形状变换得到 Q 的转置 Q<sup>T</sup> ；③ 对上述得到的矩阵 P 和矩阵 Q<sup>T</sup>求矩阵相乘"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82129a0f5b775d48"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0032, -0.0106],\n",
      "        [-0.0138,  0.0139],\n",
      "        [-0.0042,  0.0075]], dtype=torch.float64)\n",
      "tensor([[-3.4224e-05, -1.3588e-02],\n",
      "        [-1.5247e-02, -5.9312e-03],\n",
      "        [ 7.3446e-03,  3.1417e-03],\n",
      "        [-1.5942e-02, -1.8460e-03]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Step1: N(0,0.01)创建\n",
    "# 法一：\n",
    "P = torch.randn(3,2)*0.01\n",
    "Q = torch.randn(4,2)*0.01\n",
    "# 法二：\n",
    "import numpy as np\n",
    "P = torch.tensor(np.random.normal(0,0.01,size=(3,2)))\n",
    "Q = torch.tensor(np.random.normal(0,0.01,size=(4,2)))\n",
    "print(P)\n",
    "print(Q)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T09:10:10.841730Z",
     "start_time": "2024-03-28T09:10:10.825439Z"
    }
   },
   "id": "8d5080a6c21ba37c"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.4224e-05, -1.5247e-02,  7.3446e-03, -1.5942e-02],\n",
      "        [-1.3588e-02, -5.9312e-03,  3.1417e-03, -1.8460e-03]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: 对Q进行转置\n",
    "Qt = Q.t()\n",
    "print(Qt)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T09:15:54.060552Z",
     "start_time": "2024-03-28T09:15:54.013419Z"
    }
   },
   "id": "b08d89019ba6f8e4"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4332e-04,  1.3260e-05, -9.3918e-06, -3.2112e-05],\n",
      "        [-1.8853e-04,  1.2864e-04, -5.8008e-05,  1.9509e-04],\n",
      "        [-1.0175e-04,  1.8909e-05, -6.9747e-06,  5.2432e-05]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(torch.mm(P,Qt))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T09:16:16.451993Z",
     "start_time": "2024-03-28T09:16:16.437448Z"
    }
   },
   "id": "e1683cde3d3f292c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.3 给定公式 y3 = y1 + y2 = x<sup>2</sup> + x<sup>3</sup>，且 x=1。利用学习所得到的Tensor的相关知识，求y3对x的梯度，即dy3/dx。 <br>&ensp;&ensp;要求在计算过程中，在计算x<sup>3</sup>时中断梯度的追踪，观察结果并进行原因分析.<br>提示：可使用 with torch.no_grad(),举例：<br> with torch.no_grad():<br>&ensp;&ensp;y=x*5"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1fbcfa54575f560"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# 计算 y1\n",
    "y1 = x ** 2\n",
    "\n",
    "# 中断梯度追踪\n",
    "with torch.no_grad():\n",
    "    # 计算 y2\n",
    "    y2 = x ** 3\n",
    "\n",
    "# 计算 y3\n",
    "y3 = y1 + y2\n",
    "\n",
    "# 计算梯度dy3/dx，反向传播\n",
    "y3.backward()\n",
    "\n",
    "#输出x的梯度值\n",
    "print(x.grad)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T09:32:38.384796Z",
     "start_time": "2024-03-28T09:32:38.378253Z"
    }
   },
   "id": "9e3ea3ad2f4308b7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "对如上结果进行分析：\n",
    "如果不中断y2=x^3的梯度传递，则最终 y3对x 的梯度值应为5，而实际为 2，则说明 y2 在计算过程中没有被追踪梯度，因此 y3 的梯度只与 y1 相关。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4e15d44dbf3ae29"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 二、动手实现 logistic 回归"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38c3ad8b5f8f2fe3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 从 0 实现"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e12c66d730ccbb5e"
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "import torch\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T09:35:58.320593Z",
     "start_time": "2024-03-28T09:35:58.318159Z"
    }
   },
   "id": "7990df69e262e6a8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 利用torch.nn实现"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b6820c46afaa1ef"
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T09:35:51.446800Z",
     "start_time": "2024-03-28T09:35:51.429493Z"
    }
   },
   "id": "7c8c734f302a4ac1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 三、动手实现 softmax 回归"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db3600f9f7fb11dd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ec0fa5784014b7f2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
